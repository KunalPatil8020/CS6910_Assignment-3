{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq_2_seq_with_attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "8G98q_IuPWDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "!pip install wandb\n",
        "import wandb\n",
        "from keras.layers import SimpleRNN, LSTM, GRU, Dense, Dropout, TimeDistributed, Lambda, Activation, Reshape,\n",
        "Softmax, Multiply, AdditiveAttention, Concatenate, Add, RepeatVector\n",
        "import tarfile\n",
        "import random\n",
        "from keras.optimizers import RMSprop, Adam, SGD\n",
        "import os\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "from math import log\n",
        "from numpy import array, argmax\n",
        "import sklearn"
      ],
      "metadata": {
        "id": "PXHgyMP6PdgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WANDB Login"
      ],
      "metadata": {
        "id": "ixAB54giPmGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login\n",
        "from wandb.keras import WandbCallback\n",
        "wandb.init(project=\"Assignment-3\", entity=\"kunal_patil\")"
      ],
      "metadata": {
        "id": "7CPgTIamPn3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the data"
      ],
      "metadata": {
        "id": "YgOduNVWPugl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!gdown https://drive.google.com/file/d/1NDNWaV_4H-BrDn_2cxll2RvTvsW5prqM/view?usp=sharing\n",
        "!unzip \"/content/hi.zip\""
      ],
      "metadata": {
        "id": "K5TZGTMyVYKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data processing"
      ],
      "metadata": {
        "id": "1aV4m0HVRu-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_data(data):\n",
        "    input = []\n",
        "    input_char = []\n",
        "    target = []\n",
        "    target_char = []\n",
        "\n",
        "    for i in data:\n",
        "        input.append(i)\n",
        "        a=data[i]\n",
        "        b=list(i)\n",
        "        target.append(a)\n",
        "        input_char = list(set(input_char + b))\n",
        "        target_char = list(set(target_char + b(a)))\n",
        "\n",
        "    num_input = len(input_char)\n",
        "    num_target = len(target_char)\n",
        "\n",
        "    input_char_map=[]\n",
        "    for i, char in enumerate(input_char):\n",
        "      a=(char,i+1)\n",
        "      input_char_map.append(a)\n",
        "    input_char_map=dict(input_char_map)\n",
        "\n",
        "    target_char_map=[]\n",
        "    for j, c in enumerate(target_char):\n",
        "      b=(c,j+1)\n",
        "      target_char_map.append(b)\n",
        "    input_char_map=dict(input_char_map)\n",
        "\n",
        "    encoder_seq=[]\n",
        "    for wr in input:\n",
        "      encoder_seq.append(len(wr))\n",
        "\n",
        "    decoder_seq=[]\n",
        "    for wr in target:\n",
        "      decoder_seq.append(len(wr))\n",
        "\n",
        "    enco_len= max(encoder_seq)\n",
        "    deco_len= max(decoder_seq)\n",
        "\n",
        "    return input_char, target_char, num_input, num_target, enco_len, deco_len, input_char_map, target_char_map"
      ],
      "metadata": {
        "id": "fmzGzRPoVjbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(data, enco_len, deco_len, num_input, num_target):\n",
        "    L = len(data.keys())\n",
        "    target = [\"\\t\"+data[i]+\"\\n\" for i in data]\n",
        "    input = [j for j in data]\n",
        "\n",
        "    enco_in_arr = np.zeros((L, enco_len, num_input + 1))\n",
        "    deco_in_arr = np.zeros((L, deco_len, num_target + 1))\n",
        "    deco_out_arr = deco_in_arr\n",
        "\n",
        "    for i in range(L):\n",
        "        in = input[i]\n",
        "        tar = target[i]\n",
        "\n",
        "        for j, ch in enumerate(in):\n",
        "            enco_in_arr[i, j, input_char_map[ch]] = 1.0\n",
        "\n",
        "\n",
        "    return input, target, enco_in_arr, deco_in_arr, deco_out_arr"
      ],
      "metadata": {
        "id": "4gk0JgSnR4P3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_char,num_input, input_char_map, num_target, enco_len, deco_len,target_char, target_char_map = extract_data(training_map)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdGVmAnMVoto",
        "outputId": "e6bb0f18-022a-4162-8332-29863c4d3962"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique input characters: 26\n",
            "Number of unique output characters: 64\n",
            "\n",
            "Maximum sequence length for the input: 20\n",
            "Maximum sequence length for the output: 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pad_deco_in_arr = tensorflow.keras.preprocessing.sequence.pad_sequences(deco_in_arr, maxlen=max_deco_seqlength)\n",
        "pad_deco_out_arr = tensorflow.keras.preprocessing.sequence.pad_sequences(deco_out_arr, maxlen=max_deco_seqlength)\n",
        "pad_enco_in_arr = tensorflow.keras.preprocessing.sequence.pad_sequences(enco_in_arr, maxlen=max_enco_seqlength)\n",
        "\n",
        "pad_val_enco_in_arr = tensorflow.keras.preprocessing.sequence.pad_sequences(val_enco_in_arr, maxlen=enco_len)\n",
        "pad_val_deco_in_arr = tensorflow.keras.preprocessing.sequence.pad_sequences(val_deco_in_arr, maxlen=deco_len)\n",
        "pad_val_deco_out_arr = tensorflow.keras.preprocessing.sequence.pad_sequences(val_deco_out_arr, maxlen=deco_len)\n",
        "\n",
        "pad_deco_out_arr_a = np.zeros((pad_deco_out_arr.shape[0], deco_len, tar))\n",
        "for i,j in enumerate(pad_deco_out_arr.shape[0],deco_len):\n",
        "  pad_deco_out_arr_a[i][j][pad_deco_out_arr[i][j]] = 1.0\n",
        "\n",
        "pad_val_deco_out_arr_oh = np.zeros((pad_val_deco_out_arr.shape[0], deco_len, tar))\n",
        "for i,j in enumerate(pad_val_deco_out_arr.shape[0],deco_len):\n",
        "  pad_val_deco_out_arr_a[i][j][pad_val_deco_out_arr[i][j]] = 1.0"
      ],
      "metadata": {
        "id": "LHCQSznhX6nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "find an embedding using the Embedding layer\n"
      ],
      "metadata": {
        "id": "J2vXkKwBZEWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deco_in_arr = np.argmax(deco_in_arr)\n",
        "test_deco_in_arr = np.argmax(test_deco_in_arr)\n",
        "val_deco_in_arr = np.argmax(val_deco_in_arr)\n",
        "\n",
        "enco_in_arr = np.argmax(enco_in_arr)\n",
        "test_enco_in_arr = np.argmax(test_enco_in_arr)\n",
        "val_enco_in_arr = np.argmax(val_enco_in_arr)"
      ],
      "metadata": {
        "id": "8wKjz6zeV7vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention class define"
      ],
      "metadata": {
        "id": "U34rYggCf75y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(Attention, self).__init__()\n",
        "        self.W_a = self.add_weight(units)\n",
        "        self.U_a = self.add_weight(units)\n",
        "        self.V_a = self.add_weight(Dense(1))\n",
        "\n",
        "    def call(self, query, values):\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "        \n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W_a(query_with_time_axis) + self.U_a(values)))\n",
        "\n",
        "\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "f7-fZCjWWKPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Model"
      ],
      "metadata": {
        "id": "0Zw-TtHqffZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def def_model(cell_type, num_encoder_layers, num_decoder_layers, input_embedding_size, drop_out_ratio, beam_size):\n",
        "\n",
        "    enco_input = keras.Input(shape=(None, ))\n",
        "    enco_embed = keras.layers.Embedding(num_input, embedding_size)(enco_input)\n",
        "\n",
        "    encoder_states = list()\n",
        "    for j in range(len(enc_latent_dims)):\n",
        "        if cell_type == \"SimpleRNN\":\n",
        "            encoder_outputs, state = keras.layers.SimpleRNN(enc_latent_dims[j], dropout = drop_out_ratio, return_state = True, return_sequences = True)(encoder_outputs)\n",
        "            encoder_states = [state]\n",
        "        if cell_type == \"LSTM\":\n",
        "            encoder_outputs, state_h, state_c = keras.layers.LSTM(enc_latent_dims[j], dropout = drop_out_ratio, return_state = True, return_sequences = True)(encoder_outputs)\n",
        "            encoder_states = [state_h,state_c]\n",
        "        if cell_type == \"GRU\":\n",
        "            encoder_outputs, state = keras.layers.GRU(enc_latent_dims[j], dropout = drop_out_ratio, return_state = True, return_sequences = True)(encoder_outputs)\n",
        "            encoder_states = [state]\n",
        "\n",
        "\n",
        "    ## DECODER\n",
        "    deco_input = keras.Input(shape=(None, ))\n",
        "    deco_embed = keras.layers.Embedding(num_target , embedding_size)(deco_input)\n",
        "\n",
        "\n",
        "    for j in range(len(enc_latent_dims)):\n",
        "        if cell_type == \"SimpleRNN\":\n",
        "            decoder_outputs, state = keras.layers.SimpleRNN(enc_latent_dims[j], dropout = drop_out_ratio, return_state = True, return_sequences = True)(encoder_outputs)\n",
        "            decoder_states = [state]\n",
        "        if cell_type == \"LSTM\":\n",
        "            decoder_outputs, state_h, state_c = keras.layers.LSTM(enc_latent_dims[j], dropout = drop_out_ratio, return_state = True, return_sequences = True)(encoder_outputs)\n",
        "            decoder_states = [state_h,state_c]\n",
        "        if cell_type == \"GRU\":\n",
        "            decoder_outputs, state = keras.layers.GRU(enc_latent_dims[j], dropout = drop_out_ratio, return_state = True, return_sequences = True)(encoder_outputs)\n",
        "            decoder_states = [state]\n",
        "\n",
        "    decoder_dense = keras.layers.Dense(num_target, activation=\"softmax\")\n",
        "    model = keras.Model([enco_input, deco_input], decoder_dense(decoder_outputs))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "VL5XsFRyWMMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def infer_lstm(num_cell):\n",
        "    h_enco, c_enco = layer.output\n",
        "\n",
        "    enco_model = keras.Model(enco_inp, [h_enco, c_enco])\n",
        "\n",
        "    # Input to the decoder\n",
        "    deco_inputs = model.input[1]\n",
        "    deco_embed_layer  = model.layers\n",
        "\n",
        "    deco_in_state = [keras.Input(shape=(num_cells))]\n",
        "\n",
        "    deco_outputs, state_h_dec, state_c_dec = deco_lstm(deco_embed(deco_inputs))\n",
        "    deco_states = [state_h_dec, state_c_dec]\n",
        "\n",
        "    deco_model = keras.Model([deco_inputs] + deco_in_phase, [decoder_dense(decoder_output)] + deco_state)\n",
        "\n",
        "    return enco_model, deco_model\n",
        "\n",
        "\n",
        "def trans_lstm(input):\n",
        "        out-num = decoder_model.predict([target_sequence] + encoded_hidden_cell_states)\n",
        "\n",
        "        sample = np.argmax(out_num[-1, :])\n",
        "\n",
        "        tar_seq = np.zeros((batch_size))\n",
        "\n",
        "        while ch,i in (char_index):\n",
        "            deco_w[i] = deco_w[i] + char_map[i_index]\n",
        "            tar_seq[i, 0, i_index] = 1.0\n",
        "\n",
        "        tar_seq = np.argmax(tar_seq)\n",
        "    return tar_seq"
      ],
      "metadata": {
        "id": "ryhV1sUVyHkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train with wandb"
      ],
      "metadata": {
        "id": "RpwWeaDg_1V2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_wandb():\n",
        "    defaults = dict(\n",
        "    cell_type = \"LSTM\",\n",
        "    embedding_size = 64,\n",
        "    hidden_layer_size = 128,\n",
        "    num_encoder_layers = 1,\n",
        "    batch_size = 64,\n",
        "    num_decoder_layers = 1,\n",
        "    drop_out_ratio = 0,\n",
        "    epochs = 10,\n",
        "    optimizer = 'rmsprop',\n",
        "    )\n",
        "    config = wandb.config\n",
        "\n",
        "    hyperparameters = {\n",
        "    \"embedding_size\" : config.embedding_size,\n",
        "    \"num_encoder_layers\" : config.num_encoder_layers,\n",
        "    \"num_decoder_layers\" : config.num_decoder_layers,\n",
        "    \"hidden_layer_size\" : config.hidden_layer_size,\n",
        "    \"cell_type\" : config.cell_type,\n",
        "    \"drop_out_ratio\": config.drop_out_ratio,\n",
        "    \"in_char_size\": len(english_alpha2index),\n",
        "    \"out_char_size\": len(hindi_alpha2index),\n",
        "    \"input_len\": 25,\n",
        "    }\n",
        "    \n",
        "\n",
        "    model = def_model(num_cells, cell_type, layers, embedding_size, drop_out_ratio)\n",
        "\n",
        "    f = \"infer_model\" + str(layers)\n",
        "\n",
        "    enco_model, deco_model = globals()[f](model, cells)\n",
        "\n",
        "    val_infe_acc = np.mean(np.array(outputs) == np.array(ground_truths))\n",
        "    print(\"Validation accuracy= {} %\".format(val_infe_acc*100.0))\n",
        "    run = \"cell_{}_layer_{}_emb_{}_dp_{}_bm_{}\".format(cell, layers, embed_size, drop_out_ratio, size_of_beam)\n",
        "    print(run)\n",
        "\n",
        "    wandb.run.finish()"
      ],
      "metadata": {
        "id": "tYayev3rWZIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''sweep_config = {\n",
        "    'method' : 'bayes',\n",
        "    'metric' : {\n",
        "        'name' : 'word_acc',\n",
        "        'goal' : 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'embedding_size': {\n",
        "            'values': [64,128,96]\n",
        "        },\n",
        "        'num_encoder_layers': {\n",
        "            'values': [1,2,3]\n",
        "        },\n",
        "        'num_decoder_layers': {\n",
        "            'values': [1,2,3]\n",
        "        },\n",
        "        'hidden_layer_size': {\n",
        "            'values': [64,128,256]\n",
        "        },\n",
        "        'cell_type' : {\n",
        "            'values': ['LSTM','GRU','SimpleRNN']  \n",
        "        },\n",
        "        \n",
        "        'drop_out_ratio': {\n",
        "            'values': [0,0.2,0.4]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [64]\n",
        "        },\n",
        "        'epochs': {\n",
        "            'values': [25]\n",
        "        },\n",
        "        'optimizer':{\n",
        "            'values': ['adam','rmsprop']\n",
        "        }\n",
        "        \n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"Assignment-3\", entity=\"kunal_patil\")\n",
        "wandb.agent(sweep_id, train_wandb)'''"
      ],
      "metadata": {
        "id": "M5Uw7jCD_9Rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Best Model"
      ],
      "metadata": {
        "id": "NKl5UMe_Cq1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(num_cells, cell_type, layers, embed_size, drop_out_ratio):\n",
        "    model = def_model(num_cells, cell_type, layers, embed_size, drop_out_ratio)\n",
        "\n",
        "    history = model.fit(\n",
        "            [enco_in_arr, deco_in_arr],\n",
        "            deco_out_arr,\n",
        "            batch_size = 64,\n",
        "            epochs = 20,\n",
        "            verbose = 2\n",
        "            )\n",
        "\n",
        "    model.save(\"best_model.h5\")\n",
        "\n",
        "    f = \"infer_model\" + str(layers)\n",
        "\n",
        "    enco_model, deco_model = globals()[f](model, cells)\n",
        "\n",
        "    val_infe_acc = np.mean(np.array(outputs) == np.array(ground_truths))\n",
        "\n",
        "    \n",
        "    plt.plot(history.history[\"val_accuracy\"])\n",
        "    plt.plot(history.history[\"accuracy\"])\n",
        "    plt.title(\"Accuracy vs epoch\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(history.history[\"val_loss\"])\n",
        "    plt.plot(history.history[\"loss\"])\n",
        "    plt.title(\"Loss vs epoch\")\n",
        "    plt.show()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "eOEyZJFdC2Tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training with the Best Hyperparameters"
      ],
      "metadata": {
        "id": "fw1RukZHC6PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_cells = 256\n",
        "cell_type = \"GRU\"\n",
        "num_layers = 2\n",
        "embedding_size = 32\n",
        "drop_out_ratio = 0.1\n",
        "num_encoder_layers = num_layers\n",
        "num_decoder_layers = num_layers\n",
        "\n",
        "model, history = train(num_cells, cell_type, layers, embedding_size, drop_out_ratio)"
      ],
      "metadata": {
        "id": "AXTLpCZuLqZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"utf.zip\"\n",
        "import matplotlib.font_manager as fm\n",
        "fontprop = fm.FontProperties(fname='Devanagari/Lohit-Devanagari.ttf')"
      ],
      "metadata": {
        "id": "UnuzLBHnWlpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = []\n",
        "\n",
        "if not os.path.exists('predictions_attention'):\n",
        "  os.mkdir('predictions_attention')\n",
        "\n",
        "with open('Test_Prediction_attention.npy', 'wb') as f:\n",
        "  np.save(f)\n",
        "df_test.to_csv(\"TTest_Prediction_attention.csv\", index=False)\n",
        " \n",
        "print(\"Test set accuracy  = {} %\".format((np.mean(np.array(outputs) == np.array(ground_truths)))*100.0))"
      ],
      "metadata": {
        "id": "6uEcrby_WpaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.to_csv('Test_Prediction_attention',index=False)"
      ],
      "metadata": {
        "id": "IW_Mn4DuWtru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv(\"Test_Prediction_attention.csv\")\n",
        "df_test.head(25)"
      ],
      "metadata": {
        "id": "ui21PFzeDMcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f, g = plt.subplots(3,3,figsize=(20,10))\n",
        "i = 0\n",
        "for i in random_indices:\n",
        "  att = np.array(row[att_w])\n",
        "  att = sample_attention.reshape((sample_attention.shape[0],sample_attention.shape[2]))\n",
        "  decode = [i for i in row['predictions']]\n",
        "  encode = [i for i in row['input_texts']]\n",
        "  plt.y(range(attention.shape[0]),fontproperties=FontProperties(fname = 'Lohit-Devanagari.ttf'))\n",
        "  plt.x(range(attention.shape[0]),test_input_words[i])\n",
        "  plt.imshow(attention.reshape(attention.shape[0],[:,len(deco_w)],cmap=\"Blues\")"
      ],
      "metadata": {
        "id": "z30v3BlbW2Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot GIF"
      ],
      "metadata": {
        "id": "cD1-WBcVbmMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 as cv\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "\n",
        "print(df_test.iloc[6])\n",
        "a = df_test.iloc[w,0]\n",
        "i=0\n",
        "f=[]\n",
        "while i<=len(df_test.iloc[6]):\n",
        "  b = attentions[df_test.iloc[6]][i]\n",
        "  j=0\n",
        "  while j<=len(df_test.iloc[6]):\n",
        "    c.set((facecolor='blue', b=b[j]))\n",
        "    j+=0\n",
        "  plt.show()\n",
        "  f.append(cv.cvtColor(cv.imread(t), cv.COLOR_BGR2RGB ))\n",
        "  i+=1  \n",
        "\n",
        "clip = ImageSequenceClip(f, fps=3)\n",
        "clip.write_gif(str(df_test.iloc[6])+'.gif', fps=3)"
      ],
      "metadata": {
        "id": "VGoan2opW5Xf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}