
# CS6910_Assignment-3

The goal of this assignment is fourfold: (i) learn how to model sequence to sequence learning problems using Recurrent Neural Networks (ii) compare different cells such as vanilla RNN, LSTM and GRU (iii) understand how attention networks overcome the limitations of vanilla seq2seq models (iv) visualise the interactions between different components in a RNN based model.

## Prerequisites
Python 3.7.10\
Numpy 1.19.5\
Tensorflow 2.4.1
## Dataset
We have used the  Dakshina dataset ("https://github.com/google-research-datasets/dakshina").
## Set up and Installation
Both vanilla_seq2seq and seq_2_seq_with_attention has been implented in Google Colab.

For vanilla_seq2seq https://github.com/KunalPatil8020/CS6910_Assignment-3/blob/main/vanilla_seq2seq.ipynb

For seq_2_seq_with_attention https://github.com/KunalPatil8020/CS6910_Assignment-3/blob/main/seq_2_seq_with_attention.ipynb 
## Visulization and Results
To see the heat map and model https://github.com/KunalPatil8020/CS6910_Assignment-3/tree/main/visualization  

For test prediction by vanilla_seq2seq https://github.com/KunalPatil8020/CS6910_Assignment-3/tree/main/predictions_vanilla   

For test prediction by seq_2_seq_with_attention https://github.com/KunalPatil8020/CS6910_Assignment-3/tree/main/predictions_attention



## Report
https://wandb.ai/kunal_patil/Assignment-3/reports/CS6910-Assignment-3-Report--VmlldzoxOTk5MTc3 